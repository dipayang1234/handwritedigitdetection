{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "        [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "        [ 0.,  0., 10., ..., 12.,  1.,  0.]]),\n",
       " 'target': array([0, 1, 2, ..., 8, 9, 8]),\n",
       " 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
       "         [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
       "         [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
       "         [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
       "         [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
       "         [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
       "         [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
       "         [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
       "         [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]),\n",
       " 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 5620\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttp://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. topic:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
       "       15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
       "       12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
       "        0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
       "       10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  5, 13,  9,  1,  0,  0,  0,  0, 13, 15, 10, 15,  5,  0,  0,\n",
       "        3, 15,  2,  0, 11,  8,  0,  0,  4, 12,  0,  0,  8,  8,  0,  0,  5,\n",
       "        8,  0,  0,  9,  8,  0,  0,  4, 11,  0,  1, 12,  7,  0,  0,  2, 14,\n",
       "        5, 10, 12,  0,  0,  0,  0,  6, 13, 10,  0,  0,  0], dtype=int32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['data'][0].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'images', 'target', 'target_names']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#basic features of the dataset\n",
    "dir(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1797"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1797"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1797"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(df['data'],df['target'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  9., ...,  5.,  0.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  0., ..., 16., 13.,  0.],\n",
       "       [ 0.,  0.,  4., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0., 13., ..., 10.,  0.,  0.]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1437"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1437"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "model=svm.SVC(gamma=0.0001,C=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.0001, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 8, 1, 8, 6, 3, 5, 7, 7, 7, 4, 2, 6, 1, 8, 9, 8, 1, 9, 9, 4, 3,\n",
       "       1, 5, 6, 9, 4, 9, 9, 3, 7, 8, 7, 9, 4, 0, 8, 2, 7, 9, 9, 6, 8, 3,\n",
       "       0, 5, 6, 9, 2, 7, 5, 0, 3, 4, 8, 0, 3, 7, 2, 4, 5, 1, 9, 5, 3, 7,\n",
       "       1, 1, 4, 5, 6, 7, 3, 5, 6, 5, 9, 7, 3, 0, 4, 2, 7, 2, 0, 0, 2, 7,\n",
       "       0, 9, 3, 6, 0, 6, 0, 4, 6, 6, 3, 5, 5, 6, 4, 1, 8, 8, 2, 4, 2, 9,\n",
       "       6, 2, 1, 9, 4, 1, 5, 2, 4, 0, 2, 3, 3, 8, 4, 2, 1, 2, 0, 0, 4, 0,\n",
       "       6, 9, 8, 4, 2, 4, 0, 7, 0, 2, 3, 9, 2, 4, 3, 9, 1, 3, 3, 6, 8, 4,\n",
       "       8, 1, 3, 6, 5, 3, 5, 0, 7, 6, 0, 3, 4, 1, 4, 5, 9, 9, 5, 1, 7, 2,\n",
       "       7, 0, 3, 5, 2, 6, 1, 8, 9, 9, 6, 0, 6, 5, 9, 2, 1, 0, 6, 5, 9, 3,\n",
       "       6, 2, 6, 7, 2, 3, 2, 4, 9, 8, 0, 7, 3, 7, 1, 7, 9, 8, 2, 2, 9, 4,\n",
       "       5, 9, 0, 0, 0, 3, 3, 5, 4, 4, 2, 8, 1, 1, 5, 0, 0, 7, 4, 7, 8, 4,\n",
       "       1, 9, 1, 3, 8, 5, 1, 5, 4, 7, 0, 9, 7, 9, 4, 7, 6, 2, 8, 3, 3, 4,\n",
       "       4, 3, 1, 0, 6, 8, 9, 5, 5, 8, 2, 9, 4, 9, 3, 3, 0, 6, 6, 7, 2, 9,\n",
       "       0, 1, 0, 6, 9, 1, 6, 4, 9, 7, 9, 6, 7, 8, 1, 7, 9, 3, 2, 4, 8, 4,\n",
       "       2, 8, 0, 0, 7, 3, 5, 3, 3, 3, 9, 2, 9, 2, 1, 2, 9, 1, 7, 6, 4, 1,\n",
       "       0, 0, 2, 2, 5, 3, 3, 1, 8, 7, 3, 9, 3, 2, 8, 9, 8, 9, 4, 1, 5, 0,\n",
       "       3, 0, 9, 6, 7, 2, 5, 2])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9861111111111112"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [7]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Prediction: ',model.predict([df.data[-12]]))\n",
    "plt.imshow(df.images[-12],cmap=plt.cm.gray_r,interpolation=\"Nearest\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
